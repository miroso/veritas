####################################################################################################
BIG DATA
####################################################################################################
What is forecasting?
Can you explain what does data exploration mean?
What is monitoring and how it is used?
How do recommendation system work?
What is user profiling?
What do you understand under Influence Analytics?
What are the steps for implementation a Portfolio Optimization?
What is MapReduce concept (from spark)?


####################################################################################################
COMPUTER VISION
####################################################################################################
How do you compute distances between objects in one image?
What is triangulation and how can you apply it in computer vision?
What is HOG and linear SVM better than other classical object detection approaches?
What is LBP (local binary patterns) feature extractor?
How does HOG work exactly?
What classical object detector and classifiers do you know?
Why the sum of all the elements in Edge detection kernel is zero?


####################################################################################################
C++
####################################################################################################

What are the 5 types of inheritance in C++?
How is C++ code generated? What is a generator? what is a linker?
What are the three access class specifiers in C++?
What do the letters in OOP signify?
What is a class?
What is an object?
What is encapsulation?
What is data abstraction?
What is inheritance?
What is an advantage of inline functions?
What makes a class an abstract class?
What is unsigned?
What is a virtual function?
What types of inheritances do we have?
What is multiple inheritance?
What is hierarchical inheritance?
What is virtual inheritance and how why does it solve the diamond problem?
What are compile time constructs? One example is sizeof, others?
What is the arrow-type operator?
What is the auto-operator?
What is size_t type?
How is dynamic memory being used?
What is Qt?
How does "struct" work in C++?
How do exceptions in C++ work? Give an example.
What is legacy code?
Write down an example template class.
What is  "#pragma" and how can it be used?
What is the role of protected?
What is a test fixture in google tests and how do you use it (check it in google primer)?
What is the role of the keyword "delete"?
What is a "dependency injection" in C++?
What are memory leaks in C++?
How does "virtual" work?


####################################################################################################
DEEP LEARNING
####################################################################################################
What kind of metrics do we have? How do plot these?
What are the axis of a ROC curve? How do you explain a ROC curve?
Why do you use momentum? How does it stop?
How do you perform unit test for your neural network model?
Explain forward learning algorithm and backpropagation.
What is a DeepDream project?
How would you describe reinforcement learning? What is special about the NN architecture?
How does convolution in CNNs work? How is defined mathematically?
What is valid and same convolution?
CNN: How are the layers organized? How do you get the features/layer? Number of filters?
What is the role of the bias? Does is have something in common with bias towards a certain class?
How does sgd work - combine the code with from pyimagesearch with andrew NG. Why do you multiply the error with the derivate (d = error * sigmoid_deriv(preds))
​Describe hinge loss computation (DL p92)
Describe cross entropy loss computattion (DL p92)
What are the 4 main steps of classificstion (DL p82-83)
What is the bias trick (DL p99)
How do you update weights in momentum in comparison to SGD?
How does momentum works?
How do you stop the momentum so it does not overshoot?
Why is the advantage of nesterow accelerations?
What is the difference between L1 regularization, L2 regularization (commonly called “weight decay”), and Elastic Net? Equations with normal weight update? p. 115-117
What are explicit (dropout) an implicit regularization techniques (data augmentation and early stopping)?
Having a few points - how does a curve looks like when underfit, overfit, tuned?
Explain cross-entropy?
How can the loss over entire training set can be written?
What are standard building blocks of a NN architecture?
Draw a simple NN architecture (e.g. pp. 124) - what about the bias?
How can you describe the output of a NN mathematically? (p. 124)
What types of activation functions do you know?
What types of NN architectures do you know (e.g. feed-forward, recurrent, inception?, ...)?
Explain this notation:3-2-3-2 feedforward network (p. 128)
What is the Hebbs principle?
Draw and, or, xor with states | add picture (p. 130)
What is the architecture of a Perceptron network | Explain how it works (p. 130)
What is the delta rule in perceptron learning
Why do they take the derivate in backpropagation?
How does backpropagation work | explain forward and backward pass (Andrew Ng - machien learning ^ https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
What are the current eval metrics, e.g. recall, f1 score or others?
What are the four ingredients of a neural neutwork? (p. 164)
Whats the problem with constant initialization?
What are the different types for weight initialization?
What are fully-connected layers?
Explain local invariance and compositionality regarding CNNs. (p. 169)
What are image convolutions? [In terms of deep learning, an (image) convolution is an element-wise multiplication of two matrices
followed by a sum.]
What do they do?
Why do we use them?
How do we apply them to images?
And what role do convolutions play in deep learning?
What is the difference between cross-correlation and convolution
What is a kernel (p. 172)
Why should the values of a kernel sum up to 0 [1?]?
Checkout how the convolution really works in the script
How is cv2.filter2D implemented in comparison to covolve.py?
What is space invariance and local connectivity in CNNs?
What is an activation map
How is the number of channels changing across the CNN (p.182)
How many weights per kernel do you have - how do you calculate it?
What are the methods to reduce spatial input size in CNNs? [Stride + The primary function of the POOL layer is to progressively reduce the spatial size (i.e., width
and height) of the input volume. Doing this allows us to reduce the amount of parameters and
computation in the network – pooling also helps us control overfitting.] (p.189 )
[
    This result is in contrast to when we set decay=0.01 / 40 (right) and obtain a much nicer
learning plot (and not to mention, higher accuracy). By using learning rate decay we can often not
only improve our classification accuracy but also lessen the effects of overfitting, thereby increasing
the ability of our model to generalize
]
What is the padding equation? (p. 186)
What is the difference between same and valid padding?
How you calculate the size of a net after pooling (p. 187)
Can pooling overlap?
Pooling (p.188) [note: In their 2014 paper, Striving for Simplicity: The All Convolutional Net, Springenberg et al. [127]
recommend discarding the POOL layer entirely and simply relying on CONV layers with a larger
stride to handle downsampling the spatial dimensions of the volume.]
What are fully-connected layers?
Write down a simple architecture of a CNN (p. 188 bottom)
What is batch normalization - how does it work and where do we apply it? [First introduced by Ioffe and Szegedy in their 2015 paper, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift [128],]
Where do you put batch normalization into your network? (p. 190)
What is dropout? (p. 190)
Have a look at the architectures (p. 192)
Are CNNs Invariant to Translation, Rotation, and Scaling? (p. 194)
What can you do aganst over and underfitting (coursera)
What are the characteristics of a vggnet
[
    The VGG family of Convolutional Neural Networks can be characterized by two key components:
    1. All CONV layers in the network using only 33 filters.
    2. Stacking multiple CONV => RELU layer sets (where the number of consecutive CONV =>
RELU layers normally increases the deeper we go) before applying a POOL operation.
]
Why is it wise to use pooling layers deeper or not at all? What are the disatvantages of a pooling layer?
[
    In both ShallowNet and LeNet we have applied a series of CONV => RELU => POOL layers. However,
in VGGNet, we stack multiple CONV => RELU layers prior to applying a single POOL layer.
Doing this allows the network to learn more rich features from the CONV layers prior to downsampling
the spatial input size via the POOL operation.
]
What are the advantages of learning rate schedulers?
[
    Instead, what we can do is decrease our
learning rate, thereby allowing our network to take smaller steps – this decreased rate enables
our network to descend into areas of the loss landscape that are "more optimal" and would have
otherwise been missed entirely by our larger learning rate.
]
What is weight scheduling and when is it applied?
[
    We can, therefore, view the process of learning rate scheduling as:
1. Finding a set of reasonably “good” weights early in the training process with a higher learning
rate.
2. Tuning these weights later in the process to find more optimal weights using a smaller
learning rate.

after each batch update
]
What types of learning rate schedulers do we have? (p. 242)
How would you calculate the weight decay? (p. 242)
What Are Underfitting and Overfitting? {p.251-2}
[
    In this chapter we reviewed underfitting and overfitting. Underfitting occurs when your model
is unable to obtain sufficiently low loss on the training set. Meanwhile, overfitting occurs when
the gap between your training loss and validation loss is too large, indicating that the network is
modeling the underlying patterns in the training data too strong.
]
How can you combat overfittign and underfitting
Effect on learning rate on over and underfitting
What if Validation Loss Is Lower than Training Loss?
What packages would you use for visualizing neural networks_?
[
    graphviz and pydot
]
What is spatial pyramid pooling?
What is otsu's thresholding technique?
Explain the haar cascade algorithm
[
    The Haar cascade algorithm is capable of detecting objects in images, regardless of their
location and scale. Perhaps most intriguing (and relevant to our application), the detector can run
in real-time on modern hardware. In fact, the motivation of behind Viola and Jones’ work was to
create a face detector.
]
What is the difference between haar cascade and hog?
What is data augmentation {PB: 15}
What types of image augmentation do you know?
Why do you use aumentation?
{
    However, as we’ll find out later in this chapter, data augmentation can help dramatically reduce
overfitting, all the while ensuring that our model generalizes better to new input samples. Furthermore,
when working with datasets where we have too few examples to apply deep learning, we
can utilize data augmentation to generate additional training data, thereby reducing the amount of
hand-labeled data required to train a deep learning network.
}
What types of transfer learning do you know?
{
    In general, there are two types of transfer learning when applied to deep learning for computer
vision:
1. Treating networks as arbitrary feature extractors. [add picture PB p.35] -> What is a feature extractor
2. Removing the fully-connected layers of an existing network, placing new FC layer set on
top of the CNN, and fine-tuning these weights (and optionally previous layers) to recognize
object classes.
}
What is HDF5 and how is the data stored there? {PB p. 35}
What is rank-1, rank-5, and rank-N accuracy? And how do they differ from the traditional
accuracy (i.e., precision)?
{
    Rank-1 accuracy is, therefore, the percentage of
predictions where the top prediction matches the ground-truth label – this is the “standard” type of
accuracy we are used to computing: take the total number of correct predictions and divide it by
the number of data points in the dataset.
1. Step #1: Computing the class label probabilities for each input image in the dataset.
2. Step #2: Determining if the ground-truth label is equal to the predicted class label with the
largest probability.
3. Step #3: Tallying the number of times where Step #2 is true.

We can then extend this concept to rank-5 accuracy. Instead of caring only about the number
one prediction, we care about the top-5 predictions. Our evaluation process now becomes:
1. Step #1: Compute the class label probabilities for each input image in the dataset.
2. Step #2: Sort the predicted class label probabilities in descending order, such that labels
with higher probability are placed at the front of the list.
3. Step #3: Determine if the ground-truth label exists in the top-5 predicted labels from Step
#2.
4. Step #4: Tally the number of times where Step #3 is true.
Rank-5 is simply an extension to rank-1 accuracy: instead of caring about only the #1 prediction
from the classifier, we’ll take into account the top-5 predictions from the network.
}

{
    infos to network surgery. PB 68. First, freeze basemodel and train the head for 30-40 epochs. Then unfreeze the last conv layer. If classification accuracy continues to
improve (without overfitting), you may want to consider unfreezing more layers in the body.
At this point we should have a warm start to training, so we’ll switch over to SGD (again with
a small learning rate) and continue training. Train for about 100 epochs.
}
what is Jensen’s Inequality?
what is the difference between voting and averaging in decision trees?
{
    Dietterich’s work hinges on Jensen’s Inequality, which is known as the “diversity” or the
“ambiguity decomposition” in machine learning literature. The formal definition of Jensen’s
Inequality states that the convex combined (average) ensemble will have error less than or equal to
the average error of the individual models. It may be that one individual model has a lower error
than the average of all models, but since there is no criterion that we can use to “select” this model,
we can be assured that the average of all models will perform no worse than selecting any single
model at random. In short, we can only get better by averaging our predictions together; we don’t
have to fear making our classifier worse.
}
How many models do you train in ensemble learning for image recognition or classification?
{
    we normally only see 5-10
Convolutional Neural Networks in an ensemble – the reason is due to the fact that CNNs are much
more time-consuming and computationally expensive to train
}
Somthing about ensembles?
{
    It’s important to note that we would never jump straight to training an ensemble – we would
first run a series of experiments to determine which combination of architecture, optimizer, and
hyperparameters yields the highest accuracy on a given dataset.
Once you’ve reached this optimal set of combination, we would then switch over to training
multiple models to form an ensemble. Training an ensemble as your very first experiment is considered
premature optimization as you don’t know what combination of architecture, optimizer, and
hyperparameters will work best for your given dataset.
}
Check: Train 1, get M for free paper

Optimization algos:
{
    Adagrad?
    Adadelta?
    RMSProp?
    Adam?
    Nadam?

    * add names and papers to them
    * check Karpathy’s excellent optimization method notes [26]
}
Why have all state-of-the-art architectures been trained with SGD and not with ADAM?
{
    But why is this? We can clearly see the benefits in algorithms that apply adaptive learning rates
such as RMSprop and Adam – networks can converge faster. However, the speed of convergence,
while important, is not the most important factor – hyperparameters still win out. If you cannot
tune the hyperparameters to a given optimizer (and associated model), your network will never
obtain reasonable accuracy.
}
How many data samples per class should we usually have?
{
    1000 - 5000
}
According to Andrew Ng, what is the recipy of training p. 93 PB
Notes to transfer learning in PB S. 96
Why is loading images in batches ineffvicient and what is the alternative?
What means the option "stratify" in train_test_split?
HDF5 infos:
{
    For example, if we defined a dataset generator that loaded images sequentially from disk, we
would need N read operations, one for each image. However, by placing our dataset of images
into an HDF5 dataset, we can instead load batches of images using a single read. This action
dramatically reduces the number of I/O calls and allows us to work with very large image datasets.
}
What was the problem with AlexNet in Chapter 9 of SB?
How can GoogLenet go so deep without getting lost in the parameters?
What is special about GoogLeNet?
{
First, the model architecture is tiny compared to AlexNet and VGGNet ( 28MB for the weights
themselves). The authors are able to obtain such a dramatic drop in network architecture size (while
still increasing the depth of the overall network) by removing fully-connected layers and instead
using global average pooling. Most of the weights in a CNN can be found in the dense FC layers –
if these layers can be removed, the memory savings are massive.
Secondly, the Szegedy et al. paper makes usage of a network in network or micro-architecture
when constructing the overall macro-architecture. Up to this point, we have seen only sequential
neural networks where the output of one network feeds directly into the next. We are now going
to see micro-architectures, small building blocks that are used inside the rest of the architecture,
where the output from one layer can split into a number of various paths and be rejoined later.
Specifically, Szegedy et al. contributed the Inception module to the deep learning community, a
building block that fits into a Convolutional Neural Network enabling it to learn CONV layers with
multiple filter sizes, turning the module into a multi-level feature extractor.
}
What is the inception module?
How do 1 x 1 convolutions work and learn local features?
What is functional API regarding kears models?
What is the difference between sequential and non-sequential models in keras?
MiniGoogLeNet - how do you get the filters corectly?
Describe the ResNet residual modules (3 types)
What is the concept: "identity mappings" used in ResNet? {174 PB}
What is the so-called pre-activation module in ResNet?



######
Network Architectures:
GoogLeNet, SqueezeNet, ResNet50, Inception V3, Xception, ImageNet, MobileNet, ShallowNet, VGG16, VGG19, miniVGGNet (p. 230 | 278)



#####
papers:
Simonyan and Zisserman’s work [100],
Gradient-Based Learning Applied to Document Recognition [19]. LeNet
Zhang et al.’s 2017
publication, Understanding Deep Learning Requires Re-Thinking Generalization [42].
Springenberg et al. [41] from PB
Identity Mappings in Deep Residual Networks [33]

####################################################################################################
INFORMATICS
####################################################################################################
How do you compute run-time complexity. Explain with examples.


####################################################################################################
MACHINE LEARNING
####################################################################################################
What is Fourier Transform and how does it work?
What is PCA and how does it work?
How is regression defined?
What is a regressor?
How do I recognize a regression problem?
What is bootstrapping and how does it work?
How are Markov models defined and how can they be applied, e.g. in NLP?
What is fuzzy logic and where can it be applied?
What are evolutionary algorithms good for?
How would you fit a function using sin/cos? How would the fit look like, when training with few/multiple training points?
What types of cross validations do we have?
What is the difference between bagging and boosting
How does adaboost work - boosting in general?
extend bagging boosting with examples (e.g. random forrest)
how does hyperopt work?
why is hyperopt better than other optimizing algos?
What other optimizing stragegies do you know?


####################################################################################################
MATH
####################################################################################################
How does partial derivation work | use examples


####################################################################################################
NATURAL PHENOMENA
####################################################################################################
How is rainbow created?
How is light in a light bulb created?
How is electricity created?
How is wind created?
how does a telephone work?



####################################################################################################
PAPERS
####################################################################################################
HOG https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf



####################################################################################################
PYTHON
####################################################################################################
How would you write unit test for neural networks?
Check the following pages in order to see how it is done and implement it your own system

The medium page:
https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765
The github repo:
https://github.com/Thenerdstation/mltest

Hwo do you deploy a python egg?


####################################################################################################
TERMINAL
####################################################################################################
Get atom cheat sheet.
Get t-mux cheat sheet.
Get vim cheat sheet.
R-Sync short cuts.
How do you use multiple windows in terminal


####################################################################################################
PROJECTS
####################################################################################################

_____________________
art
do some art with GANs

_____________________
Vehicle Detection
> visualize separate filters
> compare different methods
> why is the hog better than the other classical algorithms?
> use the LBP feature extractor

_____________________
Vehicle Detection
> car detection + classifier to detect if it is a car or a truck or something bigger. This info would be useful for subsequent triangulation, i.e. we need the size estimate of the car
> create a website to the projects at one point?

_____________________
Smart mirrow with alexa
> can you make your own alexa

_____________________
azure + 2 pis recrding something + 3 pi with temperature sensor or each of the two